{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1856cf1c-2274-4eb7-ab79-e448d4ec9eb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<!-- Portada -->\n",
    "<div style=\"display: flex; justify-content: center; align-items: center; height: 100vh; font-family: Arial, sans-serif; background-color: #f4f4f4; text-align: center;\">\n",
    "    <div style=\"background: white; padding: 50px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2); width: 70%; max-width: 800px;\">\n",
    "        <h1 style=\"font-size: 32px; margin-bottom: 20px; color: #333;\">Predicci√≥n de color LAB para m√°quinas industriales</h1>\n",
    "        <h2 style=\"color: #555;\">M√°ster en Inteligencia Artificial</h2>\n",
    "        <h3 style=\"color: #555;\">Universidad Complutense de Madrid</h3>\n",
    "        <h4 style=\"color: #555;\">Fecha: <script>document.write(new Date().toLocaleDateString());</script></h4>\n",
    "        <h4 style=\"color: #555;\">Alumno: <strong>Jaume Poch Blanch</strong></h4>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "# **√çndice**  \n",
    "\n",
    "## üìå [1. Introducci√≥n](#introduccion)  \n",
    "   - [Objetivo del proyecto](#objetivodelproyecto)  \n",
    "   - [Contexto industrial](#contextoindustrial)  \n",
    "   - [Problema actual en la impresi√≥n sobre pl√°stico](#problemaactual)  \n",
    "   - [Propuesta de valor](#propuestadevalor)  \n",
    "\n",
    "## üìå [2. Estudio del problema con el equipo](#estudiodelproblemaconelequipo)  \n",
    "\n",
    "## üìå [3. Datos](#datos)  \n",
    "   - [Obtenci√≥n de datos](#obtenciondedatos)  \n",
    "   - [Representaci√≥n gr√°fica de la simulaci√≥n](#representaciongraficadelasimulacion)  \n",
    "\n",
    "## üìå [4. Procesamiento y Transformaci√≥n de Datos](#procesamientoytransformaciondedatos)  \n",
    "   - [Enriquecimiento de datos](#enriquecimientodedatos)  \n",
    "   - [Eliminaci√≥n de filas no relevantes](#eliminaciondefilasnorelevantes)  \n",
    "\n",
    "## üìå [5. An√°lisis Exploratorio de Datos (EDA)](#analisisexploratoriodedatoseda)  \n",
    "   - [Eliminar y adecuar datos](#eliminaciondevariablesinnecesarias)  \n",
    "   - [Transformaciones de datos](#transformacionesdedatos)  \n",
    "   - [Matriz de correlaci√≥n](#matrizdecorrelacion)\n",
    "   - [Divisi√≥n en conjuntos de Train/Test/Validaci√≥n](#divisionentrenuestrosconjuntos)  \n",
    "\n",
    "## üìå [6. Modelado](#modelado)  \n",
    "   - [Definici√≥n del Modelado y Selecci√≥n de M√©tricas](#definiciondelmodelado)  \n",
    "     - [M√©tricas de Evaluaci√≥n](#metricasdeevaluacion)  \n",
    "     - [Consideraciones sobre la Arquitectura](#consideracionessobrelasarquitecturas)  \n",
    "     - [Comparaci√≥n de Enfoques](#comparacionrnntransformers)  \n",
    "   - [Configuraciones y Par√°metros Globales para todos los modelos implementados](#configuracioninicial)  \n",
    "     - [Desfase y Tama√±o de Ventana](#tamanoventana)  \n",
    "     - [Reproducibilidad](#reproducibilidad)  \n",
    "     - [Optimizaciones](#optimizaciones)  \n",
    "   - [Implementaci√≥n de Modelos elegidos](#implementaciondemodelos)  \n",
    "     - [Modelos Basados en RNN (LSTM y GRU)](#rnnbasadosmodelos)  \n",
    "       - [Decisiones Clave en la Arquitectura](#decisionesclavegrulstm)  \n",
    "       - [Optimizaci√≥n y Ajuste de Hiperpar√°metros](#optimizacionhiperparametros)  \n",
    "     - [Modelos Basados en Transformers](#transformersbasadosmodelos)\n",
    "       - [Consideraciones sobre la Arquitectura del Modelo](#consideracionessobrelasarquitecturas)  \n",
    "       - [Optimizaci√≥n y Ajuste de Hiperpar√°metros](#optimizacionhiperparametros_transformers)\n",
    "         \n",
    "## üìå [7. Evaluaci√≥n y Comparaci√≥n de Modelos](#evaluacionycomparaciondemodelos)  \n",
    "   - [Observaciones Globales en Modelos GRU, LSTM y Transformers](#observacionesglobalesgrulstmtransformers)  \n",
    "   - [Evaluaci√≥n GRU](#evaluaciongru)  \n",
    "     - [Resultados de los experimentos GRU](#resultadosexperimentogru)\n",
    "     - [Conclusiones GRU](#conclusionesgru)\n",
    "   - [Evaluaci√≥n LSTM](#evaluacionlstm)  \n",
    "     - [Resultados de los experimentos LSTM](#resultadosexperimentolstm)\n",
    "     - [Conclusiones LSTM](#conclusioneslstm)\n",
    "   - [Evaluaci√≥n Transformers](#evaluaciontransformers)  \n",
    "     - [Conclusiones Transformers](#conclusionestransformers)\n",
    "   - [El mejor modelo](#elmejormodelo)  \n",
    "\n",
    "## üìå [8. Conclusiones](#conclusionesymejorasfuturas)  \n",
    "   - [Conclusiones Principales del estudio actual](#conclusionesprincipalesestudioactual)  \n",
    "   - [Mejoras para las siguientes iteraciones](#mejorassiguientesiteraciones)  \n",
    "   - [Lecciones Aprendidas](#leccionesaprendidas)  \n",
    "   - [Informe Ejecutivo](#informeejecutivo)  \n",
    "\n",
    "## üìå [9. Bibliograf√≠a y Recursos](#bibliografiayrecursos)  \n",
    "\n",
    "## üìå [10. Anexos](#anexos)  \n",
    "\n",
    "---\n",
    "\n",
    "<!-- Secci√≥n 1: Introducci√≥n -->\n",
    "<a name=\"introduccion\"></a>\n",
    "# **1. Introducci√≥n**  \n",
    "\n",
    "<a name=\"objetivodelproyecto\"></a>\n",
    "## üìå **Objetivo del proyecto**  \n",
    "\n",
    "Este proyecto tiene como finalidad desarrollar un modelo de predicci√≥n de color para m√°quinas industriales de impresi√≥n sobre pl√°stico.\n",
    "\n",
    "<a name=\"contextoindustrial\"></a>\n",
    "## üìå **Contexto industrial**  \n",
    "\n",
    "La empresa para la que se desarrolla este proyecto se especializa en la venta de componentes para estas m√°quinas, en particular c√°maras que capturan diferentes partes de la impresi√≥n y las transforman en valores del espacio de color **CIELAB**. Estos valores se comparan con el color requerido por el cliente y, si exceden ciertos umbrales de tolerancia, generan una alarma para ajustar el color y mantenerlo dentro de los par√°metros aceptables.\n",
    "\n",
    "Las mediciones de color se almacenan en un archivo XML bajo el est√°ndar industrial **ISO 17972-1:2015**.\n",
    "\n",
    "<a name=\"problemaactual\"></a>\n",
    "## üìå **Problema actual en la impresi√≥n sobre pl√°stico**  \n",
    "\n",
    "Los principales desaf√≠os que enfrenta el sistema actual son:\n",
    "\n",
    "- **Ubicaci√≥n de las c√°maras:** Las m√°quinas industriales poseen diversas configuraciones, lo que implica que las c√°maras de visi√≥n se sit√∫an en diferentes posiciones y distancias respecto a los inyectores de color.\n",
    "- **Sistema reactivo en lugar de predictivo:** Actualmente, los ajustes de color solo ocurren tras detectar una desviaci√≥n, lo que genera desperdicio de material.\n",
    "- **Reducci√≥n de la merma:** Los errores de impresi√≥n generan desechos de pl√°stico innecesarios.\n",
    "- **P√©rdida de productividad:** Las intervenciones manuales para reajustar los colores interrumpen la producci√≥n.\n",
    "\n",
    "<a name=\"propuestadevalor\"></a>\n",
    "## üìå **Propuesta de valor**  \n",
    "\n",
    "Para abordar estos problemas, se busca desarrollar un modelo que pueda **predecir los valores LAB con antelaci√≥n**, permitiendo intervenir antes de que el color se salga de los umbrales permitidos. Esto proporcionar√≠a varios beneficios:\n",
    "\n",
    "- **Reducci√≥n de la merma:** Al anticipar los fallos, se minimizar√≠a la cantidad de material desechado.\n",
    "- **Mayor eficiencia y sostenibilidad:** Se reducir√≠a el impacto ambiental al disminuir el desperdicio de pl√°stico.\n",
    "- **Automatizaci√≥n del ajuste de color:** En caso de una predicci√≥n precisa, se podr√≠a implementar un sistema que ajuste los inyectores autom√°ticamente, eliminando la necesidad de intervenci√≥n humana y mejorando la productividad.\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Secci√≥n 2: Estudio del problema con el equipo -->\n",
    "<a name=\"estudiodelproblemaconelequipo\"></a>\n",
    "# **2. Estudio del problema con el equipo**\n",
    "\n",
    "Tras un proceso intensivo de reuniones y consultas con los expertos de la empresa, se han identificado y analizado los factores clave que influyen en la variaci√≥n del color durante la impresi√≥n sobre pl√°stico. Estos hallazgos permiten definir, por un lado, los retos que se deben abordar en el desarrollo del modelo predictivo y, por otro, las conclusiones que fundamentan el enfoque propuesto. A continuaci√≥n, se resumen los puntos m√°s relevantes:\n",
    "\n",
    "- **Velocidad de la m√°quina:**\n",
    "  - *Parada:* No hay producci√≥n ni movimiento, lo que permite evaluar el comportamiento del sistema en inactividad.\n",
    "  - *Puesta a punto:* Durante la fase de ajuste de par√°metros y pruebas de impresi√≥n, con velocidades entre 10 y 50 m/min.\n",
    "  - *Velocidad normal:* Operaci√≥n en condiciones de producci√≥n estable (300-350 m/min), donde se espera obtener resultados consistentes.\n",
    "  - *Velocidad m√°xima:* En escenarios de alta producci√≥n (alrededor de 500 m/min), donde la precisi√≥n puede verse comprometida.\n",
    "\n",
    "- **Medici√≥n del color LAB:**  \n",
    "  Se utilizan c√°maras que capturan los valores del espacio **CIELAB** conforme al est√°ndar definido. El equipo nos proporciona un fichero tipo en formato XML, que recoge las lecturas de los valores por el sistema de visi√≥n, permitiendo as√≠ una integraci√≥n precisa de los datos en el modelo.\n",
    "\n",
    "- **Condiciones ambientales (Humedad y Temperatura):**  \n",
    "  Aunque se reconocen como variables potencialmente influyentes, se decidi√≥ no incorporarlas en el modelo debido a la ausencia de sensores adecuados para su medici√≥n precisa.\n",
    "\n",
    "- **Patrones de impresi√≥n:**  \n",
    "  La diversidad de dise√±os y patrones impresos por cada cliente se refleja en las variaciones de los valores LAB, lo que a√±ade complejidad al an√°lisis pero tambi√©n ofrece informaci√≥n valiosa para la predicci√≥n.\n",
    "\n",
    "- **Tipo de pl√°stico:**  \n",
    "  A pesar de su impacto en el proceso de impresi√≥n, se ha optado por descartar este factor en el modelado sint√©tico, debido a la alta complejidad que implica su correcta simulaci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Secci√≥n 3: Datos -->\n",
    "<a name=\"datos\"></a>\n",
    "# **3. Datos**\n",
    "\n",
    "> Para m√°s detalles espec√≠ficos sobre esta fase de generaci√≥n de datos ver los notebooks [**02_generador_datos_sinteticos.ipynb** y **01_DatosMeteorologicos.ipynb**](#anexos).\n",
    "\n",
    "<a name=\"obtenciondedatos\"></a>\n",
    "## üìå **Obtenci√≥n de datos**\n",
    "\n",
    "Para lo obtenci√≥n de datos:\n",
    "\n",
    "| **Proceso**                          | **Descripci√≥n** | **Referencia** |\n",
    "|--------------------------------------|---------------|---------------|\n",
    "| **Incorporaci√≥n de datos meteorol√≥gicos** | Datos de [AEMET](https://www.aemet.es/). | **01_DatosMeteorologicos.ipynb** ([Anexo](#anexos)) |\n",
    "| **Generaci√≥n de datos sint√©ticos:** | | **02_generador_datos_sinteticos.ipynb** [(Anexo)](#anexos) |\n",
    "| - **Discretizar el tiempo** | Divisi√≥n del d√≠a en \"n\" intervalos. | Apartado **Discretizador de tiempo** |\n",
    "| - **A√±adir velocidad** | Considera aceleraciones, estados y ruido. | Apartado **Velocidad de la m√°quina** |\n",
    "| - **A√±adir color LAB** | Generaci√≥n del color seg√∫n par√°metros. | Apartado **C√°lculo de color LAB** |\n",
    "\n",
    "<a name=\"representaciongraficadelasimulacion\"></a>\n",
    "## **Representaci√≥n gr√°fica de la simulaci√≥n**\n",
    "\n",
    "La representaci√≥n gr√°fica de los [datos sint√©ticos con los que trabajar√© se puede encontrar aqu√≠](https://github.com/laperl/MasterIA/blob/main/Resultados/RepresentacionDatosSinteticos.jpg).\n",
    "\n",
    "Cada ejecuci√≥n del notebook **\"02_generador_datos_sinteticos.ipynb\"** genera gr√°ficos interactivos (utilizando la librer√≠a *plotly*) que ilustran la evoluci√≥n de la velocidad y del color a lo largo del tiempo. Estos gr√°ficos, que se abren en una ventana separada para permitir zoom y exploraci√≥n detallada, muestran la variabilidad y el comportamiento de la m√°quina, simulando diferentes escenarios de operaci√≥n.\n",
    "\n",
    "> **NOTA IMPORTANTE:** Para poder obtener los mismos resultados, se han anexado los ficheros generados en una ejecuci√≥n en los archivos adjuntos (\"02_datos_limpios - BCK_ENTREGA.pkl\" y \"02_datos_limpios_sin_parada - BCK_ENTREGA.pkl\"). De esta forma, se puede reproducir el experimento, ya que al volver a ejecutar el notebook, los datos generados ser√°n diferentes.\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Secci√≥n 4: Procesamiento y Transformaci√≥n de Datos -->\n",
    "<a name=\"procesamientoytransformaciondedatos\"></a>\n",
    "# **4. Procesamiento y Transformaci√≥n de Datos**\n",
    "\n",
    "<a name=\"enriquecimientodedatos\"></a>\n",
    "## üìå **Enriquecimiento de datos**\n",
    "\n",
    "Inicialmente, el dataset contiene las siguientes columnas:\n",
    "\n",
    "| **Variable**       | **Descripci√≥n**                                                                                                   |\n",
    "|--------------------|-------------------------------------------------------------------------------------------------------------------|\n",
    "| **√çndice**         | Datetime con subdivisiones del d√≠a. Cada fila representa una medici√≥n en un instante determinado.                 |\n",
    "| **Color LAB**      | Tres columnas que contienen los valores medidos en el espacio de color LAB.                                        |\n",
    "| **Color LAB base** | Tres columnas con el color requerido por el cliente (valor de referencia).                                         |\n",
    "| **Velocidad**      | Indica el estado de la m√°quina: \"parada\", \"puesta\", \"normal\" o \"alta\". En estado \"parada\", el valor es nulo.         |\n",
    "\n",
    "Dado que los modelos iniciales basados √∫nicamente en estas variables mostraron resultados poco satisfactorios, se procedi√≥ a enriquecer el dataset mediante las siguientes transformaciones:\n",
    "\n",
    "| **Proceso**                          | **Descripci√≥n** | **Referencia** |\n",
    "|--------------------------------------|---------------|---------------|\n",
    "| **C√°lculo del indicador Œîùê∏** | Diferencia entre el color medido y el color base usando la f√≥rmula **CIE76**. No se integr√≥ en el modelo final. | - |\n",
    "| **Identificaci√≥n del patr√≥n de impresi√≥n** | Se a√±ade un identificador √∫nico para cada patr√≥n de impresi√≥n. | Apartado **Enriquecimiento de los datos** ([**02_generador_datos_sinteticos.ipynb** anexo](#anexos)) |\n",
    "| **Transformaci√≥n del momento del d√≠a** | Se aplica seno y coseno a la hora para capturar periodicidad. | Apartado **Enriquecimiento de los datos** ([**02_generador_datos_sinteticos.ipynb** anexo](#anexos)) |\n",
    "| **Duraci√≥n de la parada anterior** | Se calcula y guarda la duraci√≥n de cada bloque de paradas antes de eliminarlas. | Apartado **Enriquecimiento de los datos** ([**02_generador_datos_sinteticos.ipynb** anexo](#anexos)) |\n",
    "| **Tiempo acumulado en cada estado** | Contador de tiempo continuo en cada estado antes de un cambio. | Apartado **Enriquecimiento de los datos** ([**02_generador_datos_sinteticos.ipynb** anexo](#anexos)) |\n",
    "\n",
    "> **NOTA IMPORTANTE:** Para poder obtener los mismos resultados, se han a√±adido los datos generados de base en el repositorio. De esta forma, se puede reproducir el experimento, ya que al volver a ejecutar el notebook, los datos generados ser√°n diferentes.\n",
    "\n",
    "<a name=\"eliminaciondefilasnorelevantes\"></a>\n",
    "## üìå **Eliminaci√≥n de filas no relevantes**\n",
    "\n",
    "Una vez enriquecido el dataset, se procede a su depuraci√≥n para optimizar la calidad de las predicciones:\n",
    "\n",
    "- **Supresi√≥n de filas con estado \"parada\":**  \n",
    "  Debido a que en los periodos en que la m√°quina est√° parada no se generan valores de color relevantes, se eliminan estas filas. Sin embargo, para no perder la informaci√≥n acerca de los paros, se ha a√±adido la columna **\"duracion_parada_anterior\"** en la primera fila posterior a cada bloque de paradas.\n",
    "\n",
    "- **Incorporar la secuencia temporal (seq_time):**  \n",
    "  La eliminaci√≥n de las filas con estado \"parada\" provoca saltos en la secuencia temporal. Para solucionar este problema y mantener la continuidad necesaria para modelos secuenciales (como las RNN), se introduce la columna **\"seq_time\"**, que reordena y numeriza de forma consecutiva las filas restantes.\n",
    "\n",
    "Este proceso de enriquecimiento y depuraci√≥n de datos ha permitido transformar el dataset inicial en una estructura mucho m√°s informativa y adecuada para el modelado predictivo, preservando tanto la secuencia temporal como la informaci√≥n relevante sobre la operaci√≥n de la m√°quina. Los detalles t√©cnicos y la implementaci√≥n completa de estos pasos se pueden consultar en los notebooks anexos mencionados. \n",
    "\n",
    "---\n",
    "\n",
    "<!-- Secci√≥n 5: An√°lisis Exploratorio de Datos (EDA) -->\n",
    "<a name=\"analisisexploratoriodedatoseda\"></a>\n",
    "# **5. An√°lisis Exploratorio de Datos (EDA) y transformaciones**\n",
    "\n",
    "> Para m√°s detalles espec√≠ficos sobre esta fase de transformaci√≥n y an√°lisis se encuentran en el notebook [**03_AnalisisDeDatos.ipynb**](#anexos).\n",
    "\n",
    "Despu√©s de los pasos previos de generaci√≥n de datos, el dataset cuenta con las siguientes columnas:\n",
    "\n",
    "| **Categor√≠a**              | **Variables** |\n",
    "|---------------------------|----------------------------------------------------------------|\n",
    "| **Datos Meteorol√≥gicos**  | `tmin`, `tmed`, `tmax`, `horatmin`, `horatmax`, `hrMin`, `hrMedia`, `hrMax`, `horaHrMin`, `horaHrMax` |\n",
    "| **Datos Operativos y de Color** | `estado`, `velocidad`, `L`, `L_base`, `A`, `A_base`, `B`, `B_base`, `Delta_E` |\n",
    "| **Datos Enriquecidos**     | `patron_id`, `hour_sin`, `hour_cos`, `duracion_parada_anterior`, `tiempo_en_estado`, `seq_time` |\n",
    "\n",
    "\n",
    "<a name=\"eliminaciondevariablesinnecesarias\"></a>\n",
    "## üìå **Eliminar y adecuar datos**\n",
    "\n",
    "\n",
    "Para optimizar el procesamiento se ha optado por los siguientes cambios:\n",
    "\n",
    "| **Acci√≥n** | **Descripci√≥n** | **Variables Afectadas** |\n",
    "|------------|----------------|------------------------|\n",
    "| **Eliminar Datos Meteorol√≥gicos** | No se utilizar√°n en el modelo, por lo que se descartan. | `tmin`, `tmed`, `tmax`, `horatmin`, `horatmax`, `hrMin`, `hrMedia`, `hrMax`, `horaHrMin`, `horaHrMax` |\n",
    "| **Eliminar Variables de Referencia de Color** | Se usaban solo para calcular ŒîE, sin aportar valor directo a la predicci√≥n. | `L_base`, `A_base`, `B_base`, `Delta_E` |\n",
    "| **Variables Conservadas** | Conjunto final de variables seleccionadas para la predicci√≥n. | `estado`, `velocidad`, `L`, `A`, `B`, `patron_id`, `hour_sin`, `hour_cos`, `duracion_parada_anterior`, `tiempo_en_estado`, `seq_time` |\n",
    "| **Cambio de √çndice** | Se usa `seq_time` en lugar del √≠ndice datetime para preservar la secuencia despu√©s de eliminar filas con `estado = 'parada'`. | `seq_time` como nuevo √≠ndice |\n",
    "\n",
    "<a name=\"transformacionesdedatos\"></a>\n",
    "## üìå **Transformaciones de datos**\n",
    "\n",
    "A continuaci√≥n se muestra un resumen en formato tabular de las distintas **etapas de transformaci√≥n** de los datos:\n",
    "\n",
    "| **Etapa**                                 | **Variables Afectadas**                                                 | **Descripci√≥n**                                                                                                                                                | **Detalles/Referencias**                                                                                                     |\n",
    "|-------------------------------------------|--------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **1. Transformaciones logar√≠tmicas**      | - `duracion_parada_anterior` <br/> - `tiempo_en_estado`                 | Se aplica logaritmo para reducir la asimetr√≠a (skew) en la distribuci√≥n de dichas variables.                                                                   | Ver ‚ÄúTransformaciones logar√≠tmicas‚Äù en [03_AnalisisDeDatos.ipynb](#anexos).                                                 |\n",
    "| **2. Escalado y normalizaci√≥n (num√©ricas)** | **Con StandardScaler:** <br/> - `velocidad` <br/> - `L`, `A`, `B` <br/> - `tiempo_en_estado` (ya log-transform) <br/><br/> **Con RobustScaler:** <br/> - `duracion_parada_anterior` (tras el log) | **StandardScaler()** se emplea para uniformar la escala y facilitar la convergencia del modelo. <br/><br/> **RobustScaler()** se utiliza para atenuar el efecto de valores at√≠picos en la variable `duracion_parada_anterior`. | Ver ‚ÄúTransformaciones con StandardScaler()‚Äù y ‚ÄúTransformaciones con RobustScaler()‚Äù en [03_AnalisisDeDatos.ipynb](#anexos). |\n",
    "| **3. Transformaci√≥n de variables categ√≥ricas** | - `patron_id` <br/> - `estado` (despu√©s de eliminar filas con `estado = 'parada'`) | Se emplea **codificaci√≥n one-hot** para evitar colinealidad y dada la baja cardinalidad de las categor√≠as, no se requieren embeddings.                          | Ver ‚ÄúTransformaciones variables categ√≥ricas con one-hot‚Äù en [03_AnalisisDeDatos.ipynb](#anexos).                             |\n",
    "\n",
    "Se pueden consultar los gr√°ficos de los [datos escalados aqu√≠](https://github.com/laperl/MasterIA/blob/main/Resultados/DatosEscalados.jpg)\n",
    "\n",
    "**Resumen del an√°lisis de datos escalados:**\n",
    "\n",
    "- **El escalado es adecuado** y mantiene las estructuras originales sin distorsiones severas.  \n",
    "- **L_scaled muestra una distribuci√≥n bimodal**, mientras que **A_scaled y B_scaled son multimodales**, lo que sugiere la necesidad de t√©cnicas avanzadas como embeddings si se usan Transformers.  \n",
    "- **Velocidad escalada tiene picos bien definidos**, reflejando los estados operativos de la m√°quina, pero puede requerir balanceo para evitar sesgos.  \n",
    "- **Tiempo en estado y duraci√≥n de parada presentan sesgos**, con la √∫ltima mostrando una gran concentraci√≥n en un rango espec√≠fico y colas largas, lo que podr√≠a afectar la predicci√≥n. As√≠ y todo, en un inicio se ha decidido de conservarlas de esta manera\n",
    "\n",
    "<a name=\"matrizdecorrelacion\"></a>\n",
    "## üìå **Matriz de correlaci√≥n**\n",
    "\n",
    "Aunque los modelos basados en LSTM y Transformers pueden capturar relaciones complejas incluso con variables correlacionadas, se realiz√≥ un an√°lisis para detectar posibles redundancias.  \n",
    "El an√°lisis mostr√≥ que no existen correlaciones muy fuertes que puedan afectar negativamente al modelo.\n",
    "\n",
    "La [matriz de correlaci√≥n tambi√©n se puede visualizar aqu√≠](https://github.com/laperl/MasterIA/blob/main/Resultados/MatrizCorrelacion.jpg)\n",
    "\n",
    "<a name=\"divisionentrenuestrosconjuntos\"></a>\n",
    "## üìå **Divisi√≥n en conjuntos de Train/Test/Validaci√≥n**\n",
    "\n",
    "> *Implementaci√≥n completa y descrici√≥n t√©cnica en el notebook **04_ArreglosFinalesDatos.ipynb** (ver [anexo](#anexos))*.\n",
    "\n",
    "Ahora que ya tenemos los datos preparados, los dividiremos en las diferentes muestras para ser luego tratados. Las divisiones son correlativas es decir, van en orden y las proporciones son las siguientes:\n",
    "\n",
    "- **80% para entrenamiento (Train)**\n",
    "- **10% para validaci√≥n (Validation)**\n",
    "- **10% para prueba (Test)**\n",
    "\n",
    "Es importante destacar que no se utiliza el \"shuffle\" en esta divisi√≥n, ya que se debe preservar el orden temporal para modelos secuenciales.  \n",
    "\n",
    "---\n",
    "\n",
    "<!-- Secci√≥n 6: Modelado -->\n",
    "<a name=\"modelado\"></a>\n",
    "# **6. Modelado**\n",
    "\n",
    "> *Implementaci√≥n completa y descrici√≥n t√©cnica en el notebook **05a_ModeloRNN-DirectApproach.ipynb** y **05b_Transformers.ipynb** (ver [anexo](#anexos))*.\n",
    "\n",
    "<a name=\"definiciondelmodelado\"></a>\n",
    "## üìå **Definici√≥n del Modelado y Selecci√≥n de M√©tricas**\n",
    "\n",
    "<a name=\"metricasdeevaluacion\"></a>\n",
    "### **M√©tricas de Evaluaci√≥n Utilizadas**\n",
    "\n",
    "El objetivo del modelo es **predecir valores num√©ricos** que representan los colores **L, A y B** en un instante de tiempo futuro. Dado que no se trata de un problema de clasificaci√≥n, sino de **regresi√≥n**, las m√©tricas seleccionadas deben medir la precisi√≥n de los valores predichos.\n",
    "\n",
    "El equipo ha determinado que la m√©trica principal ser√° el **Error Cuadr√°tico Medio (MSE - Mean Squared Error)**, ya que penaliza m√°s los errores grandes y proporciona estabilidad en la evaluaci√≥n. Adicionalmente, tambi√©n se calcular√° el **Error Absoluto Medio (MAE - Mean Absolute Error)** como m√©trica complementaria.\n",
    "\n",
    "<a name=\"consideracionessobrelasarquitecturas\"></a>\n",
    "### **Consideraciones sobre la Arquitectura de los Modelos**\n",
    "\n",
    "En el proceso de impresi√≥n, hay un desfase entre el momento en que se aplica la tinta y el instante en que el sensor mide el color final. Como cada m√°quina tiene un desfase distinto, se ha configurado el modelo para que este valor sea un **par√°metro ajustable**.\n",
    "\n",
    "<a name=\"comparacionrnntransformers\"></a>\n",
    "### **Comparaci√≥n de Enfoques**\n",
    "\n",
    "Se evaluaron diferentes enfoques para abordar el problema, comparando sus ventajas y desventajas:\n",
    "\n",
    "| **Enfoque**               | **Descripci√≥n**                                                        | **Ventajas**                                                                     | **Limitaciones en Series Temporales**                                                                                     |\n",
    "|---------------------------|------------------------------------------------------------------------|-----------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Regresi√≥n Lineal**      | Encuentra una relaci√≥n lineal entre variables.                         | - R√°pida y f√°cil de interpretar.                                                  | - No captura relaciones complejas ni dependencias temporales. <br/>- Tiende a subestimar la din√°mica no lineal.           |\n",
    "| **Redes Neuronales MLP**  | Redes densas que aprenden patrones no lineales a partir de datos.      | - Aproximaci√≥n vers√°til en datos bien estructurados.                              | - No maneja secuencias de forma nativa.<br/>- Precisa t√©cnicas extras (ventanas, codificaci√≥n) para retener orden temporal. |\n",
    "| **RNN (Recurrentes)**     | Redes dise√±adas para procesar datos secuenciales paso a paso.          | - Capturan relaciones temporales inmediatas.                                      | - Problema de desvanecimiento de gradiente en secuencias largas.<br/>- Menor eficacia con dependencias muy prolongadas.   |\n",
    "| **GRU**                   | Variante de RNN con puertas de control (Gated Recurrent Units).        | - M√°s eficiente que LSTM y con menos problemas de gradientes.                     | - Menos expresivo que LSTM en secuencias muy extensas.<br/>- Puede no captar patrones de muy largo plazo con igual eficacia. |\n",
    "| **LSTM**                  | RNN con memoria a largo plazo (Long Short-Term Memory).                | - Maneja mejor las dependencias largas y el gradiente.                            | - Entrenamiento m√°s lento y mayor coste computacional que GRU.<br/>- Requiere m√°s datos y potencia en secuencias muy largas. |\n",
    "| **Transformers**          | Modelo basado en autoatenci√≥n (*self-attention*).                      | - Excelente para dependencias de amplio alcance.<br/>- Muy eficiente en paralelo. | - Alto consumo de memoria en secuencias largas (complejidad cuadr√°tica).<br/>- Menos pr√°ctico sin t√©cnicas como *sparse attention*. |\n",
    "| **Wave (p. ej. WaveNet)** | Arquitecturas creadas para se√±ales continuas (audio) y convoluciones.  | - Capturan patrones a m√∫ltiples escalas.<br/>- √ötiles en generaci√≥n de se√±ales.   | - Adaptaci√≥n a series no-audio no siempre trivial.<br/>- Elevado consumo de recursos si los datos son muy heterog√©neos.   |\n",
    "| **DeepSeek/GPT**          | Modelos preentrenados (p. ej. GPT) que pueden adaptarse con *fine-tuning*. | - Aprovechan el *transfer learning* de grandes corpus.<br/>- Capturan relaciones a largo plazo. | - Requieren recursos computacionales muy elevados.<br/>- No siempre reflejan bien la estructura temporal num√©rica (ciclos, estacionalidad). |\n",
    "\n",
    "**Nota**:  \n",
    "- En el caso de **Wave** (WaveNet, WaveRNN), aunque su dise√±o inicial est√° muy orientado a se√±ales de audio, pueden reutilizarse en **otros entornos de serie temporal** con modificaciones espec√≠ficas en la arquitectura de convoluciones dilatadas y en los mecanismos de upsampling/downsampling.  \n",
    "- En el caso de **DeepSeek/GPT**, la mayor complejidad radica en alinear los datos num√©ricos o de serie temporal con el **formato de entrada textual** (o tokenizado) t√≠pico de GPT, as√≠ como en contar con **recursos computacionales** suficientes para un *fine-tuning* prolongado.\n",
    "- Tambi√©n se ha pensado en sistemas h√≠bridos donde por ejemplo se usen Transformers mas una capa de GRU para llegar a un resultado m√°s optimizado.\n",
    "\n",
    "**Decisi√≥n Final**\n",
    "\n",
    "Se decidi√≥ utilizar **GRU/LSTM y Transformers**, ya que son modelos dise√±ados para capturar dependencias temporales y han demostrado ser efectivos en tareas de predicci√≥n secuencial. Adem√°s se pueden entrenar en tiempos y computo razonables. No se excluye en un futuro probar otras aproximaciones en el caso que estas que se van a analizar no funcionen bien.\n",
    "\n",
    "<a name=\"configuracioninicial\"></a>\n",
    "## üìå **Configuraciones y Par√°metros Globales para todos los modelos implementados**\n",
    "\n",
    "Para probar diferentes enfoques, se utiliz√≥ **TensorFlow para los modelos LSTM/GRU** y **PyTorch para Transformers**.\n",
    "\n",
    "<a name=\"tamanoventana\"></a>\n",
    "### **Desfase y Tama√±o de Ventana**\n",
    "- Se estableci√≥ un **desfase de 4 pasos**: cuando la m√°quina imprime en el instante *t*, el color objetivo se mide en *t+4*.\n",
    "- Se fij√≥ el **tama√±o de la ventana en 30** para ambos enfoques (LSTM/GRU y Transformers). Esta elecci√≥n se basa en que, con los datos disponibles, una ventana de este tama√±o parece apropiada y equilibrada para realizar comparaciones.\n",
    "\n",
    "**Importancia del Tama√±o de la Ventana**\n",
    "\n",
    "| Enfoque         | Ventanas Peque√±as | Ventanas Grandes |\n",
    "|----------------|------------------|------------------|\n",
    "| **Redes Recurrentes (LSTM/GRU)** | Pueden no capturar suficiente contexto. | Pueden a√±adir ruido y dificultar el entrenamiento. |\n",
    "| **Transformers** | Reducen carga computacional pero pueden perder contexto. | Capturan m√°s informaci√≥n pero aumentan el costo computacional. |\n",
    "\n",
    "**Conclusi√≥n:** Se encontr√≥ que un **tama√±o de ventana de 30** proporciona un buen equilibrio entre precisi√≥n y eficiencia.\n",
    "\n",
    "<a name=\"reproducibilidad\"></a>\n",
    "### **Reproducibilidad**\n",
    "\n",
    "Para asegurar que los experimentos sean **reproducibles**, he utilizado una configuraci√≥n espec√≠fica de par√°metros, incluyendo **semillas de aleatoriedad**. Esto significa que, aunque algunos procesos en aprendizaje autom√°tico pueden generar resultados diferentes cada vez que se ejecutan, al establecer estas semillas garantizo que los modelos se entrenen siempre con los mismos valores iniciales, permitiendo obtener resultados consistentes.\n",
    "\n",
    "En este caso, he aplicado semillas de aleatoriedad en varias librer√≠as clave:\n",
    "\n",
    "- **TensorFlow y Pytorch**: Para controlar c√≥mo se inicializan los pesos de la red neuronal y otros procesos internos.\n",
    "- **Random (Python)**: Para cualquier operaci√≥n que dependa de n√∫meros aleatorios, como la divisi√≥n de los datos en entrenamiento y prueba.\n",
    "- **CUDA y cuDNN**: Son tecnolog√≠as utilizadas para acelerar el entrenamiento con GPU. Sin fijar la aleatoriedad en estos sistemas, el uso de m√∫ltiples n√∫cleos podr√≠a generar ligeras variaciones en los resultados.\n",
    "\n",
    "Gracias a esta configuraci√≥n, cualquier persona que ejecute el mismo c√≥digo deber√≠a obtener los **mismos resultados**, lo que es fundamental para comparar modelos y evaluar mejoras de manera objetiva.\n",
    "\n",
    "<a name=\"optimizaciones\"></a>\n",
    "### **Optimizaciones**\n",
    "\n",
    "**Callbacks**\n",
    "\n",
    "Para todos los modelos GRU, LSTM, Transformers se utilizaron las siguientes estrategias de optimizaci√≥n:\n",
    "\n",
    "1. **Early Stopping** para evitar sobreajuste.  \n",
    "2. **ReduceLROnPlateau** para ajustar din√°micamente el *learning rate*.  \n",
    "3. **Time Callback** para medir la duraci√≥n del entrenamiento.  \n",
    "\n",
    "\n",
    "<a name=\"implementaciondemodelos\"></a>\n",
    "## üìå **Implementaci√≥n de los Modelos elegidos**\n",
    "\n",
    "<a name=\"rnnbasadosmodelos\"></a>\n",
    "### üìå **Modelos Basados en RNN (LSTM y GRU)**\n",
    "\n",
    "> *Implementaci√≥n completa, entrenamientos y descrici√≥n t√©cnica en el notebook **05a_ModeloRNN-DirectApproach.ipynb** (ver [anexo](#anexos))*.\n",
    "\n",
    "\n",
    "<a name=\"decisionesclavegrulstm\"></a>\n",
    "### **Decisiones Clave en la Arquitectura**\n",
    "\n",
    "- Reciben una **ventana de 30 pasos** como entrada.\n",
    "- Utilizan **dos capas LSTM/GRU** para extraer patrones temporales.\n",
    "- La segunda capa devuelve solo el √∫ltimo estado, prediciendo el color en *t+4*.\n",
    "- Se a√±aden **Batch Normalization y Regularizaci√≥n L2** para mejorar estabilidad y evitar sobreajuste.\n",
    "\n",
    "- **¬øLSTM o GRU?** Se evaluaron ambas arquitecturas y no se encontraron diferencias significativas en el rendimiento.\n",
    "- **¬øPor qu√© no usar Seq2Seq?** Aunque se prob√≥, el modelo Seq2Seq a√±adi√≥ complejidad innecesaria y aument√≥ el tiempo de entrenamiento, sin mejorar las predicciones. Dado que la tarea no requiere generar secuencias completas, sino predecir valores en un tiempo espec√≠fico *t*, su uso no es justificado.\n",
    "- **¬øPor qu√© no RNNs simples?** Las RNN tradicionales pueden enfrentar dificultades al capturar dependencias a largo plazo debido al problema del desvanecimiento del gradiente, las descart√© sin probar aunque podr√≠an haberse considerado.\n",
    "- **¬øPor qu√© dos capas?** Se experiment√≥ con configuraciones de 1, 2 y 3 capas, determinando que una arquitectura de **dos capas** ofrec√≠a el mejor equilibrio entre complejidad y rendimiento, adem√°s de mejores resultados.\n",
    "- **¬øPor qu√© Normalizaci√≥n y Regularizaci√≥n?** Para mitigar el sobreajuste observado durante el entrenamiento, se implementaron t√©cnicas de normalizaci√≥n y regularizaci√≥n, mejorando la capacidad de generalizaci√≥n del modelo.\n",
    "- **Optimizaci√≥n con Adam y AMSGrad:** Estas t√©cnicas de optimizaci√≥n se emplearon para mejorar la estabilidad y eficiencia del proceso de entrenamiento. Con **AMSGrad** lograba tambi√©n reproducibilidad.\n",
    "\n",
    "<a name=\"optimizacionhiperparametros\"></a>\n",
    "#### **Optimizaci√≥n y Ajuste de Hiperpar√°metros**\n",
    "\n",
    "Se probaron m√∫ltiples configuraciones mediante b√∫squeda en cuadr√≠cula (*Grid Search*).  \n",
    "Las mejores configuraciones fueron:\n",
    "\n",
    "| **Par√°metro** | **Valores Evaluados** | **Configuraci√≥n Final** |\n",
    "|--------------|------------------|------------------|\n",
    "| **N√∫mero de neuronas (latent_dim)** | 16, 32 | **32** |\n",
    "| **Dropout** | 0.4 - 0.8 | **0.5** |\n",
    "| **Learning Rate** | 1e-4 a 1e-5 | **1e-4** |\n",
    "| **Batch Size** | 32, 64 | **32** |\n",
    "| **Tipo de RNN** | LSTM, GRU | **LSTM** |\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"transformersbasadosmodelos\"></a>\n",
    "### üìå **Modelos Basados en Transformers**\n",
    "\n",
    "> *Implementaci√≥n completa, entrenamientos y descrici√≥n t√©cnica en el notebook **06_AnalisiResultados_RNN.ipynb** (ver [anexo](#anexos))*.\n",
    "\n",
    "<a name=\"consideracionessobrelasarquitecturas\"></a>\n",
    "#### **Consideraciones sobre la Arquitectura del Modelo**\n",
    "\n",
    "En este caso, solo se implementa el **Encoder del Transformer**, sin utilizar un Decoder.  \n",
    "Esto se debe a que **no estamos generando una secuencia de salida**, sino prediciendo **tres valores finales** (L, A, B).\n",
    "\n",
    "üìå **Razones para usar solo el Encoder:**\n",
    "\n",
    "- **No se necesita generaci√≥n de secuencias:** El modelo debe aprender representaciones temporales y producir un √∫nico vector de salida.\n",
    "- **Reducci√≥n de costo computacional:** Usar solo el Encoder permite un entrenamiento m√°s eficiente sin afectar la precisi√≥n.\n",
    "- **Mejor manejo de relaciones a largo plazo:** El mecanismo de atenci√≥n de los Transformers captura patrones en secuencias largas sin problemas de desvanecimiento del gradiente.\n",
    "\n",
    "**Codificaci√≥n posicional**\n",
    "\n",
    "En modelos como los Transformers, los datos se procesan como secuencias de tokens (como palabras, n√∫meros o pasos temporales). Sin embargo:\n",
    "\n",
    "* **Transformers** *no entienden el orden natural de los datos* porque no procesan la informaci√≥n de izquierda a derecha como las RNN (redes recurrentes).\n",
    "* Para que el modelo pueda distinguir la posici√≥n de cada token en la secuencia (por ejemplo, \"1¬∫, 2¬∫, 3¬∫...\"), se utiliza una codificaci√≥n posicional.\n",
    "\n",
    "Se gener√≥ una **clase `CodificacionPosicional`** (que podemos ver en el notebook correspondiente) que a√±ade informaci√≥n sobre la posici√≥n de cada elemento en la secuencia usando valores basados en funciones matem√°ticas (seno y coseno).\n",
    "\n",
    "<a name=\"optimizacionhiperparametros_transformers\"></a>\n",
    "### **Optimizaci√≥n y Ajuste de Hiperpar√°metros en Transformers**\n",
    "\n",
    "Se probaron m√∫ltiples configuraciones mediante b√∫squeda en cuadr√≠cula (*Grid Search*).  \n",
    "Las mejores configuraciones fueron:\n",
    "\n",
    "| **Par√°metro** | **Valores Evaluados** | **Descripci√≥n** |\n",
    "|--------------|------------------|------------------|\n",
    "| **d_model (Dimensi√≥n del Modelo)** | 32, 64, 128 | Define el tama√±o del vector de representaci√≥n en cada token de entrada. Mayor valor aumenta la capacidad de representaci√≥n, pero tambi√©n el costo computacional. Debe ser m√∫ltiplo de `nhead`. |\n",
    "| **nhead (N√∫mero de Cabezas de Atenci√≥n)** | 2, 4, 8 | N√∫mero de cabezas en la atenci√≥n multi-cabeza. M√°s cabezas permiten aprender relaciones m√°s complejas, pero aumentan el c√≥mputo. `d_model` debe ser divisible por `nhead`. |\n",
    "| **num_layers (Capas del Encoder)** | 1, 2, 3 | Cantidad de capas Transformer apiladas. M√°s capas mejoran la captura de relaciones a largo plazo, pero aumentan el riesgo de sobreajuste y el costo computacional. |\n",
    "| **dim_feedforward (Tama√±o de la Red Feedforward)** | 64, 128, 256 | Tama√±o de la capa oculta dentro de cada bloque Transformer. Un valor mayor mejora la expresividad del modelo, pero aumenta la memoria y el tiempo de entrenamiento. |\n",
    "| **Dropout** | 0.1, 0.2, 0.3 | T√©cnica de regularizaci√≥n que desactiva neuronas aleatoriamente para evitar sobreajuste. Valores muy altos (>0.5) pueden hacer que el modelo pierda informaci√≥n √∫til. Usualmente 0.1 o 0.2 en Transformers. |\n",
    "| **Learning Rate** | 1e-4 a 1e-5 | Controla la velocidad de ajuste de los pesos del modelo. Un valor muy alto puede impedir la convergencia, y uno muy bajo puede hacer el entrenamiento demasiado lento. |\n",
    "| **Batch Size** | 32, 64 | Cantidad de ejemplos procesados en cada iteraci√≥n de entrenamiento. Batches m√°s grandes estabilizan el entrenamiento, pero requieren m√°s memoria. |\n",
    "\n",
    "> *Implementaci√≥n completa y descrici√≥n t√©cnica en el notebook **05b_Transformers.ipynb** (ver [anexo](#anexos))*.\n",
    "\n",
    "---\n",
    "\n",
    "Este apartado resume el proceso de modelado, selecci√≥n de arquitecturas, ajuste de hiperpar√°metros y evaluaci√≥n de modelos. Ahora, con los modelos entrenados, se procede a su comparaci√≥n en la siguiente secci√≥n. üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Secci√≥n 7: Evaluaci√≥n y Comparaci√≥n de Modelos -->\n",
    "<a name=\"evaluacionycomparaciondemodelos\"></a>\n",
    "# **7. Evaluaci√≥n y Comparaci√≥n de Modelos**\n",
    "\n",
    "<a name=\"observacionesglobalesgrulstmtransformers\"></a>\n",
    "## Observaciones Globales en Modelos GRU, LSTM y Transformers\n",
    "\n",
    "1. **Ampliaci√≥n del tama√±o de datos**  \n",
    "   - Inicialmente, se dispon√≠a de un total de 80.000 ejemplos sint√©ticos, pero los resultados eran insatisfactorios: el modelo tardaba en converger y no capturaba bien los patrones subyacentes.  \n",
    "   - Al ampliar el conjunto de datos a **m√°s de 200.000** ejemplos, el modelo mostr√≥ un comportamiento m√°s estable, si bien no definitivamente √≥ptimo. En escenarios reales con datos muy variables, se requerir√≠a probablemente un volumen de datos todav√≠a mayor para obtener resultados m√°s contundentes.\n",
    "\n",
    "2. **Tama√±o de ventana (Window Size)**  \n",
    "   - Se realizaron pruebas con *window sizes* de 20 y 30, sin diferencias excesivamente significativas. Sin embargo, un *window size* de **30** ofreci√≥ ligeras mejoras y, por ello, se mantuvo como la configuraci√≥n principal. Esto concuerda con la idea de que, en sistemas industriales, disponer de un contexto temporal m√°s amplio (aun sin ser enorme) puede facilitar la detecci√≥n de tendencias.\n",
    "\n",
    "3. **Registro de m√©tricas s√≥lo en el mejor modelo de cada experimento**  \n",
    "   - Se opt√≥ por registrar la *predictions_loss_mse* (u otras m√©tricas) √∫nicamente en la configuraci√≥n con mejor validaci√≥n de cada ensayo, para agilizar el proceso y focalizar el an√°lisis.  \n",
    "   - Aun as√≠, se reconoce que calcular dichas m√©tricas en todos los modelos podr√≠a arrojar m√°s luz sobre la estabilidad y consistencia del entrenamiento. En futuras iteraciones, se valora un registro m√°s exhaustivo.\n",
    "\n",
    "Evidentemente a medida que hemos ido avanzando en los entrenamientos hemos ido acotando y refinando la parametrizaci√≥n \n",
    "\n",
    "<a name=\"evaluaciongru\"></a>\n",
    "## Evaluaci√≥n GRU\n",
    "\n",
    "> *El notebook que analiza los resultados para GRU y LSTM es  **06_AnalisisResultados_RNN.ipynb** (ver [anexo](#anexos))*.\n",
    "\n",
    "<a name=\"resultadosexperimentogru\"></a>\n",
    "### Resultados de los experimentos GRU\n",
    "\n",
    "Para no llenar este apartado de resultados, se ha decidido enlazarlos a GitHub, lo que permite una mejor organizaci√≥n y claridad:\n",
    "\n",
    "| üìå **Experimento** | üìä **Tabla de Resultados** | üìà **Learning Curves** |\n",
    "|-------------------|--------------------|----------------|\n",
    "| **Experimento 1** | [üìä Tabla de resultados](https://github.com/laperl/MasterIA/blob/main/Resultados/01_GRU_TablaResultados.jpg) | - [üìà Learning Curve 1](https://github.com/laperl/MasterIA/blob/main/Resultados/01_GRU_LearningCurves_1.jpg) <br> - [üìâ Learning Curve 2](https://github.com/laperl/MasterIA/blob/main/Resultados/01_GRU_LearningCurves_2.jpg) |\n",
    "| **Experimento 2** | [üìä Tabla de resultados](https://github.com/laperl/MasterIA/blob/main/Resultados/02_GRU_TablaResultados.jpg) | - [üìà Learning Curve 1](https://github.com/laperl/MasterIA/blob/main/Resultados/02_GRU_LearningCurves.jpg) |\n",
    "| **Otros experimentos** | [üìä Tabla de resultados](https://github.com/laperl/MasterIA/blob/main/Resultados/034567_TablaResultados_GRU.jpg) | - [üìà Learning Curve 4](https://github.com/laperl/MasterIA/blob/main/Resultados/03_GRU_LearningCurves.jpg) <br> - [üìâ Learning Curve 5](https://github.com/laperl/MasterIA/blob/main/Resultados/05_GRU_LearningCurves.jpg) <br> - [üìà Learning Curve 6](https://github.com/laperl/MasterIA/blob/main/Resultados/06_GRU_LearningCurves.jpg) <br> - [üìâ Learning Curve 7](https://github.com/laperl/MasterIA/blob/main/Resultados/07_GRU_LearningCurves.jpg) |\n",
    "\n",
    "<a name=\"conclusionesgru\"></a>\n",
    "### Conclusiones GRU\n",
    "\n",
    "1. **Elecci√≥n de la tasa de aprendizaje (Learning Rate)**  \n",
    "   - Se evaluaron diferentes valores de *learning rate*, principalmente en el rango de \\(10^{-4}\\) y \\(10^{-5}\\). Se constat√≥ que **\\(10^{-5}\\)** ofrec√≠a una mejor estabilidad y evitaba la ‚Äúexplosi√≥n de gradientes‚Äù, lo que resulta especialmente relevante en un contexto de datos con alta variabilidad sint√©tica.  \n",
    "   - Se descartaron valores de *learning rate* inferiores a \\(10^{-5}\\) (por ejemplo, \\(10^{-6}\\) o \\(10^{-7}\\)) debido a su lenta convergencia y a la escasa mejora obtenida tras numerosas iteraciones.  \n",
    "   - Otras tasas intermedias, como \\(6.5 \\times 10^{-5}\\) o \\(5 \\times 10^{-5}\\), no evidenciaron un desempe√±o tan estable como el de \\(10^{-5}\\).\n",
    "\n",
    "2. **Regularizaci√≥n y presencia de sobreajuste**  \n",
    "   - En los primeros ensayos, se apreciaba un sobreajuste significativo incluso con *dropout* moderado (0.3‚Äì0.5). Ello se atribuy√≥, en gran medida, a la **aleatoriedad inherente** de los datos sint√©ticos y a que el conjunto de datos pose√≠a ‚Äúzonas‚Äù muy heterog√©neas.  \n",
    "   - **El *dropout* de 0.7** termin√≥ siendo el que proporcion√≥ una mayor capacidad de contenci√≥n del sobreajuste, ayudado por **Batch Normalization** y la **Regularizaci√≥n L2**. Esta combinaci√≥n atenu√≥ las fluctuaciones en el error de validaci√≥n y estabiliz√≥ el entrenamiento, a pesar del incremento en la dificultad de aprendizaje que supone un *dropout* elevado.  \n",
    "   - Para mejorar la generalizaci√≥n, tambi√©n se a√±adieron capas de normalizaci√≥n por lotes (*batch normalization*), que contribuyeron a moderar los picos de p√©rdida y facilitar una convergencia m√°s suave.\n",
    "\n",
    "3. **Batch Size**  \n",
    "   - Se probaron diversos *batch sizes* (16, 32, 64), observ√°ndose que **32** generaba un mejor equilibrio entre estabilidad del gradiente y velocidad de convergencia. Con *batch sizes* m√°s peque√±os, se apreciaba cierta inestabilidad, y con valores mayores, la capacidad de ajuste parec√≠a reducirse ligeramente.\n",
    "\n",
    "4. **Dimensiones internas del modelo (16, 32 o 64 neuronas)**  \n",
    "   - No se identific√≥ una tendencia inequ√≠voca a favor de un n√∫mero de neuronas claramente superior o inferior en todas las circunstancias. Por ejemplo, redes con 16 neuronas en la capa GRU pueden converger de forma estable, mientras que 32 y 64 no siempre mostraron ventajas marcadas.\n",
    "   - M√°s experimentos son necesarios para establecer, pero probablemente en el caso de datos sint√©ticos, las muestras para Train tienen que ser mucho m√°s grandes.\n",
    "\n",
    "5. **Comparaci√≥n de P√©rdidas de Entrenamiento vs. Validaci√≥n**  \n",
    "   - En diversos experimentos, se observ√≥ que la p√©rdida (loss) en validaci√≥n llegaba a ser **incluso menor** que la de entrenamiento. Una posible explicaci√≥n es la relativa homogeneidad de la porci√≥n de validaci√≥n, o bien diferencias de distribuci√≥n en los lotes.  \n",
    "   - Si la selecci√≥n del bloque de validaci√≥n contiene intervalos menos variables, el modelo puede ajustarse con mayor facilidad en dicha parte. En escenarios reales, esto subraya la importancia de garantizar una partici√≥n representativa entre entreno y validaci√≥n.\n",
    "\n",
    "**Reflexi√≥n Final**\n",
    "\n",
    "En s√≠ntesis, **los experimentos confirman que un *learning rate* reducido (\\(10^{-5}\\)) y un *dropout* relativamente alto (0.7) pueden ofrecer un balance √≥ptimo** para combatir el sobreajuste en datos sint√©ticos con alta aleatoriedad. A ello se suma la conveniencia de ampliar el volumen de muestras (por encima de 200k) y mantener un *window size* suficiente (en torno a 30 o m√°s) para capturar la din√°mica temporal. No obstante, en un escenario industrial real, **ser√≠a recomendable** complementar estos hallazgos con un mayor control de la variabilidad externa, un ajuste cuidadoso de la partici√≥n de validaci√≥n y, por supuesto, la incorporaci√≥n de datos emp√≠ricos para refinar la arquitectura y la regularizaci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"evaluacionlstm\"></a>\n",
    "## Evaluaci√≥n LSTM\n",
    "\n",
    "> *El notebook que analiza los resultados para GRU y LSTM es  **06_AnalisisResultados_RNN.ipynb** (ver [anexo](#anexos))*.\n",
    "\n",
    "<a name=\"resultadosexperimentolstm\"></a>\n",
    "### Resultados de los experimentos LSTM\n",
    "\n",
    "Para no llenar este apartado de resultados, se ha decidido enlazarlos a GitHub, lo que permite una mejor organizaci√≥n y claridad:\n",
    "\n",
    "| üìå **Experimento** | üìä **Tabla de Resultados** | üìà **Learning Curves** |\n",
    "|-------------------|--------------------|----------------|\n",
    "| **Experimento 1** | [üìä Tabla de resultados](https://github.com/laperl/MasterIA/blob/main/Resultados/01_LSTM_TablaResultados.jpg) | - [üìâ Learning Curve 1](https://github.com/laperl/MasterIA/blob/main/Resultados/01_LSTM_LearningCurves.jpg) |\n",
    "| **Experimento 2** | [üìä Tabla de resultados](https://github.com/laperl/MasterIA/blob/main/Resultados/02_LSTM_TablaResultados.jpg) | - [üìà Learning Curve 1](https://github.com/laperl/MasterIA/blob/main/Resultados/02_LSTM_LearningCurves.jpg) |\n",
    "| **Otros experimentos** | [üìä Tabla de resultados](https://github.com/laperl/MasterIA/blob/main/Resultados/034567_TablaResultados_LSTM.jpg) | - [üìà Learning Curve 2](https://github.com/laperl/MasterIA/blob/main/Resultados/03_LSTM_LearningCurves.jpg) <br> - [üìâ Learning Curve 4](https://github.com/laperl/MasterIA/blob/main/Resultados/04_LSTM_LearningCurves.jpg) <br> - [üìà Learning Curve 5](https://github.com/laperl/MasterIA/blob/main/Resultados/05_LSTM_LearningCurves.jpg) |\n",
    "\n",
    "<a name=\"conclusioneslstm\"></a>\n",
    "### Conclusiones LSTM\n",
    "\n",
    "1. **Elecci√≥n de la tasa de aprendizaje (Learning Rate)**  \n",
    "   - En el caso de LSTM, lo mismo que en GRU; no funcionaron valores bajos de learning rate, el que mejor funcion√≥ fue de nuevo **\\(10^{-5}\\)**.\n",
    "\n",
    "2. **Regularizaci√≥n y presencia de sobreajuste**  \n",
    "   - Siguen los valores dropout siendo altos aunque, los mejores modelos no necesitan de un dropout tan elevado como en el caso de GRU.\n",
    "   - Para mejorar la generalizaci√≥n, tambi√©n se a√±adieron capas de normalizaci√≥n por lotes (*batch normalization*), que contribuyeron a moderar los picos de p√©rdida y facilitar una convergencia m√°s suave.\n",
    "\n",
    "3. **Batch Size**  \n",
    "   - Al igual que con las GRU, se probaron diversos *batch sizes* (16, 32, 64), observ√°ndose que **32** generaba un mejor equilibrio entre estabilidad del gradiente y velocidad de convergencia. Con *batch sizes* m√°s peque√±os, se apreciaba cierta inestabilidad, y con valores mayores, la capacidad de ajuste parec√≠a reducirse ligeramente.\n",
    "\n",
    "4. **Dimensiones internas del modelo (16, 32 o 64 neuronas)**  \n",
    "   - No se identific√≥ una tendencia inequ√≠voca a favor de un n√∫mero de dimensiones claramente superior o inferior en todas las circunstancias. Curiosamente, a diferencia de las GRU, en este caso la tendencia del modelo ganador es con 64 dimensiones.\n",
    "   - La elecci√≥n final no est√° clara y habr√≠a que hacer m√°s pruebas, de todas maneras, seguir buscando con valores de 16 y sobretodo 64 parece lo m√°s √≥ptimo.\n",
    "\n",
    "5. **Comparaci√≥n de P√©rdidas de Entrenamiento vs. Validaci√≥n**  \n",
    "   - Sucede lo mismo que en GRU. En diversos experimentos, se observ√≥ que la p√©rdida (loss) en validaci√≥n llegaba a ser **incluso menor** que la de entrenamiento. Una posible explicaci√≥n es la relativa homogeneidad de la porci√≥n de validaci√≥n, o bien diferencias de distribuci√≥n en los lotes.  \n",
    "   - Si la selecci√≥n del bloque de validaci√≥n contiene intervalos menos variables, el modelo puede ajustarse con mayor facilidad en dicha parte. En escenarios reales, esto subraya la importancia de garantizar una partici√≥n representativa entre entreno y validaci√≥n.\n",
    "\n",
    "**Reflexi√≥n Final**\n",
    "\n",
    "No queda claro si funciona mejor LSTM o GRU, hacen falta m√°s experimentos para llegar a una conclusi√≥n, lo que est√° claro es que el error medio de MSE est√° entorno 1 lo cual no es una buena noticia.\n",
    "\n",
    "Se pueden ver la comparativa de los mejores resultados de LSTM y GRU [en este gr√°fico](https://github.com/laperl/MasterIA/blob/main/Resultados/ResultadosAgregados_GRU_LSTM.jpg).\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"evaluaciontransformers\"></a>\n",
    "## Evaluaci√≥n Transformers\n",
    "\n",
    "> *Implementaci√≥n completa y descrici√≥n t√©cnica en el notebook **05b_Transformers.ipynb** (ver [anexo](#anexos))*.\n",
    "\n",
    "Tras entrenar los modelos con **window_size de 30, offset de 4** y un **batch_size de 128** pues fue el que mejor resultado nos dio, se obtuvieron los siguientes resultados que pueden verse en Github:\n",
    "\n",
    "- [Tabla de resultados](https://github.com/laperl/MasterIA/blob/main/Resultados/TransformersResultados.jpg)\n",
    "- [Learning Curves 1](https://github.com/laperl/MasterIA/blob/main/Resultados/01_Transformers_LearningCurves_1.jpg), [Learning Curves 2](https://github.com/laperl/MasterIA/blob/main/Resultados/01_Transformers_LearningCurves_2.jpg)\n",
    "\n",
    "El mejor modelo obtuvo un resultado de **0.8989** (Loss MSE) en la muestra de **Test**.\n",
    "\n",
    "<a name=\"conclusionestransformers\"></a>\n",
    "### Conclusiones Transformers\n",
    "\n",
    "- **Necesidad de m√°s datos**  \n",
    "  Incluso con *dropout* alto, se observ√≥ sobreajuste; los *Transformers* suelen exigir un volumen de datos mayor para aprovechar al m√°ximo su mecanismo de atenci√≥n.\n",
    "\n",
    "- **Persistencia de sobreajuste**  \n",
    "  A pesar del uso de *dropout* elevados, los modelos finales no generalizan bien en validaci√≥n, indicio de que la capacidad del modelo excede la informaci√≥n disponible.\n",
    "\n",
    "- **Convergencia m√°s r√°pida con 128 y `nhead=8`**  \n",
    "  Los *Transformers* que emplean `d_model=128` y `nhead=8` tienden a converger en menos √©pocas que configuraciones mayores, presumiblemente por menor complejidad.\n",
    "\n",
    "- **Cabezas de Atenci√≥n**  \n",
    "  La configuraci√≥n de 8 cabezas (*nhead=8*) funcion√≥ mejor; incrementar el n√∫mero de cabezas podr√≠a mejorar los resultados, pero tambi√©n aumentar√≠a el coste computacional.\n",
    "\n",
    "- **N√∫mero de capas**  \n",
    "  Se evaluaron 1, 2 y 3 capas; las de 3 no aportaron mejoras y complicaron el entrenamiento, por lo que se simplific√≥ a 1 o 2 capas.\n",
    "\n",
    "- **Learning Rate heredado de GRU/LSTM**  \n",
    "  Se us√≥ el mismo valor que funcion√≥ en GRU y LSTM. *ReduceLROnPlateau* ajust√≥ din√°micamente la tasa, mostrando un comportamiento aceptable pero no exhaustivamente optimizado.\n",
    "\n",
    "- **Curvas de entrenamiento vs. validaci√≥n**  \n",
    "  La validaci√≥n suele finalizar por encima de la p√©rdida de entrenamiento, lo cual sugiere que el *Transformer* es capaz de sobreajustarse m√°s f√°cilmente que las redes recurrentes.\n",
    "\n",
    "**Reflexi√≥n Final**\n",
    "\n",
    "No emergi√≥ un modelo claramente superior con los par√°metros probados, aunque la configuraci√≥n `d_model=128, nhead=8, layers=1, dim_feedforward=64, dropout=0.6, learning_rate=1e-5` mostr√≥ un desempe√±o algo mejor. La *loss* en validaci√≥n y test sigue rondando 1, insuficiente para aplicaciones industriales. Es imprescindible investigar m√°s par√°metros y, sobre todo, **disponer de m√°s datos** para que el *Transformer* despliegue todo su potencial.\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"elmejormodelo\"></a>\n",
    "## El mejor modelo\n",
    "\n",
    "No se encontr√≥ un ganador absoluto y, por ahora, ning√∫n resultado puede llevarse a producci√≥n. Sin embargo, los **Transformers** podr√≠an superar a GRU y LSTM con datos m√°s numerosos, dado que su mecanismo de atenci√≥n retiene informaci√≥n de todo el contexto de forma m√°s directa. En cambio, **GRU y LSTM** s√≠ pueden manejar dependencias a largo plazo, pero a menudo les resulta m√°s dif√≠cil cuando la secuencia es muy extensa.\n",
    "\n",
    "Para quienes deseen comparar, los mejores resultados obtenidos en cada enfoque pueden visualizarse (**descargando los archivos, ya que GitHub no los muestra por su tama√±o**):\n",
    "\n",
    "- [LSTM](https://github.com/laperl/MasterIA/blob/main/Resultados/visualizacion_LSTM4.html)  \n",
    "- [GRU](https://github.com/laperl/MasterIA/blob/main/Resultados/02_GRU_visualizacion.html)\n",
    "- [Transformers](https://github.com/laperl/MasterIA/blob/main/Resultados/01_Transformersvisualizacion.html)  \n",
    "\n",
    "---\n",
    "\n",
    "<!-- Secci√≥n 8: Conclusiones y Mejoras Futuras -->\n",
    "<a name=\"conclusionesymejorasfuturas\"></a>\n",
    "# **8. Conclusiones**\n",
    "\n",
    "<a name=\"conclusionesprincipalesestudioactual\"></a>\n",
    "## Conclusiones Principales del estudio actual\n",
    "\n",
    "1. **Necesidad de un gran hist√≥rico de datos / mediciones frecuentes**  \n",
    "   - **Motivo**: Los modelos secuenciales (RNN, LSTM/GRU, Transformers) requieren abundantes ejemplos para capturar patrones temporales y estacionales.  \n",
    "   - **Conclusi√≥n**: Aumentar la frecuencia de medici√≥n, o bien disponer de un mayor hist√≥rico, potencia de manera sustancial la capacidad predictiva.\n",
    "\n",
    "2. **Variables clim√°ticas (humedad, temperatura) como potenciales mejoras**  \n",
    "   - **Motivo**: En entornos industriales de impresi√≥n, el clima influye en la adherencia y calidad del color.  \n",
    "   - **Conclusi√≥n**: Aunque las m√°quinas no suelen registrar dichas variables, su incorporaci√≥n en un escenario real puede brindar mayor precisi√≥n y deber√≠a considerarse medirlas si fuera factible.\n",
    "\n",
    "3. **Limitaciones de los datos sint√©ticos**  \n",
    "   - **Motivo**: Un exceso de aleatoriedad o la falta de correlaci√≥n con procesos reales distorsiona las conclusiones.  \n",
    "   - **Conclusi√≥n**: Los datos sint√©ticos sirven para prototipar, pero se recomienda realizar la validaci√≥n final en condiciones reales con datos de la propia m√°quina.\n",
    "\n",
    "4. **Modelos actuales no aptos para producci√≥n**  \n",
    "   - **Motivo**: Todav√≠a no se dispone de una validaci√≥n industrial ni de un ajuste definitivo en situaciones reales.  \n",
    "   - **Conclusi√≥n**: Las soluciones actuales constituyen solo un punto de partida; es necesario refinarlas, re-entrenarlas y validarlas con informaci√≥n proveniente de la producci√≥n real.\n",
    "\n",
    "5. **Falta de comparaci√≥n concluyente entre GRU/LSTM y Transformers**  \n",
    "   - **Motivo**: Ambos enfoques se han aplicado, pero no existe un resultado cuantitativo que identifique cu√°l es sistem√°ticamente superior.  \n",
    "   - **Conclusi√≥n**: Resulta imprescindible experimentar con m√°s datos (o datos reales) para determinar cu√°l de los dos se adapta mejor al entorno industrial.\n",
    "\n",
    "6. **Tama√±o de ventana com√∫n (30) para RNN y Transformers**  \n",
    "   - **Motivo**: Se unific√≥ el tama√±o de la secuencia para simplificar la experimentaci√≥n.  \n",
    "   - **Conclusi√≥n**: Aun cuando esta uniformidad facilita la comparaci√≥n, LSTM/GRU y Transformers interpretan el tama√±o de ventana de forma distinta. Se sugiere evaluar diferentes longitudes para optimizar resultados en cada modelo.\n",
    "\n",
    "7. **Reproducibilidad y semilla**  \n",
    "   - **Motivo**: Aunque se fijen semillas en Python, NumPy, TensorFlow o PyTorch, el hardware (por ejemplo, GPU y cuDNN) puede introducir componentes no deterministas.  \n",
    "   - **Conclusi√≥n**: Es inviable alcanzar reproducibilidad total en todos los entornos; se recomienda documentar tanto la configuraci√≥n de software como la de hardware, adem√°s de las semillas empleadas.\n",
    "\n",
    "8. **Conveniencia de seguir investigando**  \n",
    "   - **Motivo**: Ning√∫n enfoque se ha consolidado como el mejor en presencia de datos sint√©ticos y un problema de alta complejidad.  \n",
    "   - **Conclusi√≥n**: Conviene continuar explorando redes recurrentes (RNN) y Transformers para identificar la soluci√≥n m√°s adecuada en cada caso.\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"mejorassiguientesiteraciones\"></a>\n",
    "## Mejoras para las siguientes iteraciones\n",
    "\n",
    "1. **Refinamiento de la variable `duracion_parada`**  \n",
    "   - Se sugiere reemplazarla o complementarla con un indicador binario (0/1: ‚Äúviene de parada‚Äù o ‚Äúno‚Äù) para atenuar la influencia de valores at√≠picos en la escala.\n",
    "\n",
    "2. **Diversificaci√≥n de los datos sint√©ticos**  \n",
    "   - Conviene ajustar factores como el nivel de ruido, la duraci√≥n de los estados y los rangos de velocidad, a fin de aproximar mejor situaciones reales y reducir la aleatoriedad excesiva.\n",
    "\n",
    "3. **Evaluaci√≥n de optimizadores adicionales**  \n",
    "   - Se propone probar *SGD con momentum*, *RMSProp*, *AdamW*, entre otros, gestionando de forma cuidadosa la tasa de aprendizaje para evitar la explosi√≥n de gradientes.\n",
    "\n",
    "4. **Ampliaci√≥n de m√©tricas de evaluaci√≥n**  \n",
    "   - Adem√°s de MSE y MAE, podr√≠a estudiarse *RMSE*, *R2* y otras espec√≠ficas de color (p. ej., ŒîE 2000) en caso de ser prioritaria la percepci√≥n visual.\n",
    "\n",
    "5. **Revisi√≥n pormenorizada de la generaci√≥n sint√©tica**  \n",
    "   - Dedicando m√°s esfuerzo a la simulaci√≥n de cambios de color, la introducci√≥n de ciclos propios de la producci√≥n real y la disminuci√≥n del ruido se conseguir√≠an datos m√°s representativos.  \n",
    "   - En este proyecto, no se destin√≥ tanto tiempo a esa tarea porque no era el objetivo principal.\n",
    "\n",
    "6. **Optimizaci√≥n del entrenamiento en Transformers**  \n",
    "   - Utilizar *mini-batches* y *mixed precision* para reducir costes computacionales, sobre todo al ampliar el *window size*.\n",
    "\n",
    "7. **Validaci√≥n temporal escalonada**  \n",
    "   - En lugar de emplear √∫nicamente la divisi√≥n 80/10/10, resulta recomendable un procedimiento ‚Äúwalk-forward‚Äù o ‚Äúrolling window‚Äù que reproduzca mejor la din√°mica real de la producci√≥n.\n",
    "\n",
    "8. **Exploraci√≥n de enfoques modernos**  \n",
    "   - Modelos como GPT o Deepseek, pese a su orientaci√≥n inicial, podr√≠an adaptarse al entrenamiento con datos sint√©ticos y ofrecer mejoras.\n",
    "\n",
    "9. **Aplicaci√≥n de t√©cnicas avanzadas**  \n",
    "   - Una vez se disponga de un modelo suficientemente adaptable, se valorar√≠a el *transfer learning* y el *fine-tuning* para ajustar un modelo base a diferentes m√°quinas de impresi√≥n con menor necesidad de datos y tiempos de puesta en producci√≥n m√°s r√°pidos.\n",
    "\n",
    "10. **Sistemas h√≠bridos**  \n",
    "   - Se contempla la posibilidad de combinar varios enfoques, aprovechando las ventajas de cada uno.  \n",
    "   - Un ejemplo es el uso de un *Transformer encoder* para extraer caracter√≠sticas globales y una capa GRU adicional para refinar patrones de corto plazo.\n",
    "\n",
    "11. **Variaci√≥n en el desfase temporal**  \n",
    "   - Se recomienda investigar la predicci√≥n con horizontes distintos (p. ej., 2, 6 o 10 pasos) en lugar de usar √∫nicamente 4, y estudiar c√≥mo responde cada modelo ante dichos intervalos.\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"leccionesaprendidas\"></a>\n",
    "## Lecciones Aprendidas\n",
    "\n",
    "1. **Control de semillas y reproducibilidad**  \n",
    "   - Conviene establecer semillas en las librer√≠as principales (TensorFlow/PyTorch, NumPy, CUDA) y documentar la configuraci√≥n de hardware. Aun as√≠, el comportamiento no se vuelve plenamente determinista.\n",
    "\n",
    "2. **Uso de `float32` y ventajas de *mixed precision***  \n",
    "   - Emplear precisi√≥n en `float32` (o *mixed precision*) reduce la memoria en GPU y acelera el entrenamiento, en especial en redes de gran tama√±o.\n",
    "\n",
    "3. **Comparaciones entre frameworks**  \n",
    "   - Implementar modelos en PyTorch y TensorFlow simult√°neamente conlleva un sobreesfuerzo, pero aumenta la flexibilidad para elegir la herramienta m√°s conveniente.\n",
    "\n",
    "4. **Importancia del tama√±o de ventana (window size)**  \n",
    "   - Determinar la ventana m√°s adecuada depende del tipo de datos y de su periodicidad. Un an√°lisis exploratorio minucioso puede revelar ciclos diarios o semanales relevantes.\n",
    "\n",
    "5. **Inestabilidad de drivers en Windows**  \n",
    "   - El uso de CUDA/cuDNN en Windows puede presentar errores o cierres inesperados, mientras que en Linux normalmente se logra una mayor estabilidad.\n",
    "\n",
    "6. **P√©rdida de rendimiento al forzar determinismo en GPU**  \n",
    "   - Activar el modo determinista en cuDNN puede ralentizar el entrenamiento de forma significativa, lo que reduce la eficiencia en proyectos de gran escala.\n",
    "\n",
    "7. **Organizaci√≥n y registro de experimentos**  \n",
    "   - Mantener un historial detallado de los cambios en el c√≥digo, los hiperpar√°metros y los datos facilita la comparaci√≥n de resultados y evita inconsistencias.\n",
    "\n",
    "8. **Evitar un *overfitting* dr√°stico**  \n",
    "   - En este proyecto, se aplicaron tasas de *dropout* elevadas (hasta 70%) para contrarrestar la aleatoriedad sint√©tica, aunque lo m√°s efectivo ser√≠a aumentar el tama√±o de la muestra para mejorar la representatividad.\n",
    "\n",
    "9. **Early Stopping e hiperpar√°metros como factores clave**  \n",
    "   - Un *early stopping* excesivamente restrictivo puede truncar el aprendizaje; uno demasiado relajado puede llevar a sobreajuste y a errores de tipo NaN si el *learning rate* es elevado.\n",
    "\n",
    "10. **Tiempo y velocidad de proceso**  \n",
    "   - La velocidad de procesamiento determina cu√°ntas pruebas se pueden realizar en un tiempo dado. Por ello, contar con hardware potente es esencial para refinar modelos de forma r√°pida y exhaustiva.\n",
    "\n",
    "11. **Herramientas avanzadas para el an√°lisis de resultados**  \n",
    "   - Ha resultado complejo extraer conclusiones √∫nicamente con las tablas y gr√°ficos empleados. El uso de m√©todos de an√°lisis m√°s automatizados puede ahorrar tiempo y aportar mayor claridad.\n",
    "\n",
    "12. **Necesidad de mayor plazo**  \n",
    "   - Realizar todas las tareas en solo dos meses ha dificultado un desarrollo exhaustivo y menos productivo de lo esperado, pero ha servido para adquirir experiencia valiosa en la estimaci√≥n de tiempos.\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"informeejecutivo\"></a>\n",
    "## **Informe Ejecutivo**\n",
    "\n",
    "La **principal conclusi√≥n** del estudio es que se requieren **m√°s datos reales** para lograr modelos predictivos robustos en la impresi√≥n industrial sobre pl√°stico. Hasta ahora, se han empleado datos sint√©ticos con resultados prometedores, pero no aptos para la producci√≥n. El tama√±o de ventana, la metodolog√≠a de entrenamiento (RNN, LSTM, GRU o Transformers) y la regularizaci√≥n (dropout, early stopping, etc.) se han explorado para mitigar el sobreajuste y mejorar la precisi√≥n. Sin embargo, sin validaci√≥n con datos reales, no puede determinarse un enfoque ‚Äúganador‚Äù.\n",
    "\n",
    "**Puntos clave:**\n",
    "- **Ampliar hist√≥rico de datos o incrementar frecuencia de medici√≥n**: Los algoritmos secuenciales exigen grandes cantidades de informaci√≥n para aprender patrones. Cuantos m√°s datos se manejen, mas recursos se requieren.\n",
    "- **Evaluar variables ambientales**: Factores como temperatura y humedad inciden en la calidad del color y podr√≠an mejorar la predicci√≥n.  \n",
    "- **Transformers con m√°s datos**: Su potencial supera a RNN y LSTM en ciertos casos, pero requieren muestras muy extensas y potencia de c√≥mputo.  \n",
    "- **Trabajo futuro**: Ajuste y validaci√≥n real con datos de producci√≥n industrial, incorporaci√≥n de nuevas m√©tricas de color y enfoques h√≠bridos para incrementar la precisi√≥n.  \n",
    "\n",
    "En conclusi√≥n, **las investigaciones realizadas constituyen un primer paso prometedor**, pero se aconseja seguir obteniendo datos genuinos de las m√°quinas para refinar los modelos y llevarlos a un entorno productivo con confianza.\n",
    "\n",
    "---\n",
    "\n",
    "<!-- Secci√≥n 9: Bibliograf√≠a y Recursos -->\n",
    "<a name=\"bibliografiayrecursos\"></a>\n",
    "# **9. Bibliograf√≠a y Recursos**\n",
    "\n",
    "* **CIELAB**: https://es.wikipedia.org/wiki/Espacio_de_color_Lab\n",
    "* **Estandar ISO 17972-1:2015**: https://www.iso.org/standard/61500.html\n",
    "* **AEMET**: https://www.aemet.es/\n",
    "* **Comparativa de optimizadores**: https://antonio-richaud.com/biblioteca/archivo/Algoritmos-de-optimizacion-para-RN/Algoritmos-de-optimizacion-para-RN.pdf\n",
    "* **LLMs**: ChatGpt, Deepseek entre otros.\n",
    "* **Pytorch**: https://pytorch.org/\n",
    "* **Tensorflow**: https://www.tensorflow.org/?hl=es\n",
    "---\n",
    "\n",
    "<!-- Secci√≥n 10: Anexos -->\n",
    "<a name=\"anexos\"></a>\n",
    "# **10. Anexos**  \n",
    "\n",
    "- [Repositorio de c√≥digo](https://github.com/laperl/MasterIA): Contiene todo el c√≥digo fuente del proyecto, desde la adquisici√≥n de datos hasta el an√°lisis de resultados.\n",
    "\n",
    "  - [üìÑ Esta memoria](https://github.com/laperl/MasterIA/blob/main/00_MemoriaFinal.ipynb): Documento principal con la descripci√≥n del proyecto, metodolog√≠a y resultados.\n",
    "  - [üå¶Ô∏è 01_DatosMeteorologicos.ipynb](https://github.com/laperl/MasterIA/blob/main/01_DatosMeteorologicos.ipynb): Obtenci√≥n y an√°lisis de datos meteorol√≥gicos de AENET para evaluar su impacto en la predicci√≥n del color LAB.\n",
    "  - [üõ†Ô∏è 02_generador_datos_sinteticos.ipynb](https://github.com/laperl/MasterIA/blob/main/02_generador_datos_sinteticos.ipynb): Algoritmo para generar datos sint√©ticos simulando condiciones reales de impresi√≥n.\n",
    "  - [üìä 03_AnalisisDeDatos.ipynb](https://github.com/laperl/MasterIA/blob/main/03_AnalisisDeDatos.ipynb): Exploraci√≥n y transformaci√≥n de los datos para su uso en modelos de predicci√≥n.\n",
    "  - [üîÑ 04_ArreglosFinalesDatos.ipynb](https://github.com/laperl/MasterIA/blob/main/04_ArreglosFinalesDatos.ipynb): Preparaci√≥n final de los datos, incluyendo la divisi√≥n en conjuntos de entrenamiento, validaci√≥n y prueba.\n",
    "  - [ü§ñ 05a_ModeloRNN-DirectApproach.ipynb](https://github.com/laperl/MasterIA/blob/main/05a_ModeloRNN-DirectApproach.ipynb): Implementaci√≥n de modelos basados en **GRU y LSTM**, con pruebas de hiperpar√°metros.\n",
    "  - [‚ö° 05b_Transformers.ipynb](https://github.com/laperl/MasterIA/blob/main/05b_Transformers.ipynb): Desarrollo de modelos **basados en Transformers**, junto con visualizaci√≥n de resultados.\n",
    "  - [üìâ 06_AnalisiResultados_RNN.ipynb](https://github.com/laperl/MasterIA/blob/main/06_AnalisiResultados_RNN.ipynb): An√°lisis y comparaci√≥n del rendimiento de modelos **RNN (LSTM / GRU)**.\n",
    "  - [üîÆ 07_Predicciones.ipynb](https://github.com/laperl/MasterIA/blob/main/07_Predicciones.ipynb): Visualizaci√≥n de predicciones y comparaci√≥n con valores reales para evaluar la efectividad de los modelos.\n",
    "- üìπ [Link al video explicativo](https://drive.google.com/file/d/1TCzMvuu-g2_156ussxLOw_upSpv5JgsB/view?usp=drive_link): Resumen en video sobre la metodolog√≠a y conclusiones del proyecto.\n",
    "- üñºÔ∏è [Im√°genes y resultados](https://github.com/laperl/MasterIA/tree/main/Resultados): Visualizaci√≥n de datos, gr√°ficos comparativos y ejemplos de predicciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce64de0a-9373-4444-a16e-9a282f567469",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
